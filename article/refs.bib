@inproceedings{10.1145/1498759.1498761,
author = {Dean, Jeffrey},
title = {Challenges in building large-scale information retrieval systems: invited talk},
year = {2009},
isbn = {9781605583907},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1498759.1498761},
doi = {10.1145/1498759.1498761},
abstract = {Building and operating large-scale information retrieval systems used by hundreds of millions of people around the world provides a number of interesting challenges. Designing such systems requires making complex design tradeoffs in a number of dimensions, including (a) the number of user queries that must be handled per second and the response latency to these requests, (b) the number and size of various corpora that are searched, (c) the latency and frequency with which documents are updated or added to the corpora, and (d) the quality and cost of the ranking algorithms that are used for retrieval. In this talk I will discuss the evolution of Google's hardware infrastructure and information retrieval systems and some of the design challenges that arise from ever-increasing demands in all of these dimensions. I will also describe how we use various pieces of distributed systems infrastructure when building these retrieval systems. Finally, I will describe some future challenges and open research problems in this area.},
booktitle = {Proceedings of the Second ACM International Conference on Web Search and Data Mining},
keywords = {search engines, scalability},
location = {Barcelona, Spain},
series = {WSDM '09}
}

@article{HARRIS2000601,
title = {Coarse-grained information dominates fine-grained information in judgments of time-to-contact from retinal flow},
journal = {Vision Research},
volume = {40},
number = {6},
pages = {601-611},
year = {2000},
issn = {0042-6989},
doi = {https://doi.org/10.1016/S0042-6989(99)00209-6},
url = {https://www.sciencedirect.com/science/article/pii/S0042698999002096},
author = {Mike G Harris and Christos D Giachritsis},
keywords = {Fine-grained structure, Coarse-grained structure, Random dot kinematograms, Retinal flow},
abstract = {To investigate the relative importance of fine- and coarse-grained structure in the analysis of retinal flow, subjects made estimates of time-to-contact from random dot kinematograms depicting movement towards a flat, sparsely textured surface. Individual display elements moved smoothly away from each other while expanding smoothly in size. By artificially manipulating the rate at which the individual elements expanded we showed that this cue has only a small effect upon performance. When individual elements were replaced by small clusters of dots, expansion of the clusters had a similarly small effect upon performance. However, estimates of time-to-contact were possible when a single expanding cluster was presented in isolation. We conclude that both types of information are available to the subject but that estimates of time-to-contact are based primarily on coarse-grained changes in the position of image elements and that fine-grained changes in element size or position play only a minor role.}
}

@inproceedings{NEURIPS2022_c32319f4,
 author = {Kusupati, Aditya and Bhatt, Gantavya and Rege, Aniket and Wallingford, Matthew and Sinha, Aditya and Ramanujan, Vivek and Howard-Snyder, William and Chen, Kaifeng and Kakade, Sham and Jain, Prateek and Farhadi, Ali},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Koyejo and S. Mohamed and A. Agarwal and D. Belgrave and K. Cho and A. Oh},
 pages = {30233--30249},
 publisher = {Curran Associates, Inc.},
 title = {Matryoshka Representation Learning},
 url = {https://proceedings.neurips.cc/paper_files/paper/2022/file/c32319f4868da7613d78af9993100e42-Paper-Conference.pdf},
 volume = {35},
 year = {2022}
}

@article{Sanh2019DistilBERTAD,
  title={DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter},
  author={Victor Sanh and Lysandre Debut and Julien Chaumond and Thomas Wolf},
  journal={ArXiv},
  year={2019},
  volume={abs/1910.01108}
}

@online{sbert,
  title = {https://sbert.net/docs/sentence\_transformer/pretrained\_models.html\#original-models},
  url = {https://sbert.net/docs/sentence_transformer/pretrained_models.html#original-models}
}

@inproceedings{su-etal-2023-one,
    title = "One Embedder, Any Task: Instruction-Finetuned Text Embeddings",
    author = "Su, Hongjin  and
      Shi, Weijia  and
      Kasai, Jungo  and
      Wang, Yizhong  and
      Hu, Yushi  and
      Ostendorf, Mari  and
      Yih, Wen-tau  and
      Smith, Noah A.  and
      Zettlemoyer, Luke  and
      Yu, Tao",
    editor = "Rogers, Anna  and
      Boyd-Graber, Jordan  and
      Okazaki, Naoaki",
    booktitle = "Findings of the Association for Computational Linguistics: ACL 2023",
    month = jul,
    year = "2023",
    address = "Toronto, Canada",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.findings-acl.71",
    doi = "10.18653/v1/2023.findings-acl.71",
    pages = "1102--1121",
    abstract = "We introduce INSTRUCTOR, a new method for computing text embeddings given task instructions: every text input is embedded together with instructions explaining the use case (e.g., task and domain descriptions). Unlike encoders from prior work that are more specialized, INSTRUCTOR is a single embedder that can generate text embeddings tailored to different downstream tasks and domains, without any further training. We first annotate instructions for 330 diverse tasks and train INSTRUCTOR on this multitask mixture with a contrastive loss. We evaluate INSTRUCTOR on 70 embedding evaluation tasks (66 of which are unseen during training), ranging from classification and information retrieval to semantic textual similarity and text generation evaluation. INSTRUCTOR, while having an order of magnitude fewer parameters than the previous best model, achieves state-of-the-art performance, with an average improvement of 3.4{\%} compared to the previous best results on the 70 diverse datasets. Our analysis suggests that INSTRUCTOR is robust to changes in instructions, and that instruction finetuning mitigates the challenge of training a single model on diverse datasets. Our model, code, and data are available at \url{https://instructor-embedding.github.io}.",
}
